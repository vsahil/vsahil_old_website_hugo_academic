[{"authors":null,"categories":null,"content":"Hello! I am a PhD student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, Seattle. I am interested in exploring ways to make Machine Learning trustworthy. I have explored explainability and fairness aspects of Machine Learning to make it trustworthy. I hope to continue exploring this area in future.\nI majored in Electrical Engineering from IIT Kanpur and graduated with a minor in Computer Science in 2019. At IIT Kanpur, I worked on research projects in Fairness in Machine Learning and Program Analysis of Machine Learning and C code. In my free time, I enjoy reading blogs, connecting with old friends, and pop music.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vsahil.github.io/author/sahil-verma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sahil-verma/","section":"authors","summary":"Hello! I am a PhD student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, Seattle. I am interested in exploring ways to make Machine Learning trustworthy.","tags":null,"title":"Sahil Verma","type":"authors"},{"authors":["Sahil Verma","John Dickerson","Keegan Hines"],"categories":["Explainability in ML"],"content":"","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"f622e5b4e3adab600196c578a6db9b8d","permalink":"https://vsahil.github.io/publication/counterfactual-survey/","publishdate":"2020-11-27T13:37:35+05:30","relpermalink":"/publication/counterfactual-survey/","section":"publication","summary":"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.","tags":["Explainability in ML"],"title":"Counterfactual Explanations for Machine Learning: A Review","type":"publication"},{"authors":["Sahil Verma","Ruoyuan Gao","Chirag Shah"],"categories":["Fairness in ML"],"content":"","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586822400,"objectID":"afcad0cf815769b4c3ad562c3c04c0e7","permalink":"https://vsahil.github.io/publication/recommendation-fairness-definitions/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/recommendation-fairness-definitions/","section":"publication","summary":"Several recent works have highlighted how search and recommender systems exhibit bias along different dimensions. Counteracting this bias and bringing a certain amount of fairness in search is crucial to not only creating a more balanced environment that considers relevance and diversity but also providing a more sustainable way forward for both content consumers and content producers. This short paper examines some of the recent works to define relevance, diversity, and related concepts. Then, it focuses on explaining the emerging concept of fairness in various recommendation settings. In doing so, this paper presents comparisons and highlights contracts among various measures, and gaps in our conceptual and evaluative frameworks.","tags":["Fairness in ML"],"title":"Recommendation Fairness Definitions","type":"publication"},{"authors":["Sahil Verma","Roland Yap"],"categories":["Other"],"content":"","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572825600,"objectID":"92a2e64ac77da85d1557c3c1b6920e16","permalink":"https://vsahil.github.io/publication/nus-paper/","publishdate":"2020-11-27T13:37:10+05:30","relpermalink":"/publication/nus-paper/","section":"publication","summary":"Symbolic execution is a powerful technique for bug finding and program testing. It is successful in finding bugs in real-world code. The core reasoning techniques use constraint solving, path exploration, and search, which are also the same techniques used in solving combinatorial problems, e.g., finite-domain constraint satisfaction problems (CSPs). We propose CSP instances as more challenging benchmarks to evaluate the effectiveness of the core techniques in symbolic execution. We transform CSP benchmarks into C programs suitable for testing the reasoning capabilities of symbolic execution tools. From a single CSP P, we transform P depending on transformation choice into different C programs. Preliminary testing with the KLEE, Tracer-X, and LLBMC tools show substantial runtime differences from transformation and solver choice. Our C benchmarks are effective in showing the limitations of existing symbolic execution tools. The motivation for this work is we believe that benchmarks of this form can spur the development and engineering of improved core reasoning in symbolic execution engines.","tags":["Other"],"title":"Benchmarking Symbolic Execution Using Constraint Problems - Initial Results","type":"publication"},{"authors":["Jesse Michel*","Sahil Verma*","Benjamin Sherman","Michael Carbin"],"categories":["Other"],"content":"","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561161600,"objectID":"797342b20fc3fd9c20ad28029a56a81f","permalink":"https://vsahil.github.io/publication/mit-nap-workshop/","publishdate":"2020-11-27T13:37:42+05:30","relpermalink":"/publication/mit-nap-workshop/","section":"publication","summary":"Low-precision approximation of programs enables faster computation in fields such as machine learning, data analytics, and vision. Such approximations automatically transform a program into one that approximates the original output but executes much faster. At the heart of this approximation is sensitivity analysis - understanding the program's robustness to various perturbations. Sensitivity analysis provides a metric to measure how much each value in the program to produce a faster, yet accurate, approximate program. We propose NAP (Noise-based Analyzer of Programs) which provides a novel sensitivity analysis of each operator and variable in a program. NAP performs sensitivity analysis by introducing independent Gaussian noise to each value in a program (e.g., arithmetic operator and variable reference), producing a stochastic semantics of the program.","tags":["Other"],"title":"NAP: Noise-Based Sensitivity Analysis for Programs","type":"publication"},{"authors":["Sahil Verma","Julia Rubin"],"categories":["Fairness in ML"],"content":"","date":1527552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527552000,"objectID":"39b073e1a28aeb4d79504e32dbb98d29","permalink":"https://vsahil.github.io/publication/fairness-definitions/","publishdate":"2020-11-27T13:36:15+05:30","relpermalink":"/publication/fairness-definitions/","section":"publication","summary":"Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.","tags":["Fairness in ML"],"title":"Fairness Definitions Explained","type":"publication"},{"authors":["Sahil Verma","Subhajit Roy"],"categories":["Other"],"content":"","date":1503273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503273600,"objectID":"5eacd4c918a096ca9eaaa6c0254239ee","permalink":"https://vsahil.github.io/publication/sdb-wolverine1/","publishdate":"2020-11-27T13:37:02+05:30","relpermalink":"/publication/sdb-wolverine1/","section":"publication","summary":"We present Wolverine, an integrated Debug-Repair environment for heap manipulating programs. Wolverine facilitates stepping through a concrete program execution, provides visualizations of the abstract program states (as box-and-arrow diagrams) and integrates a novel, proof-directed repair algorithm to synthesize repair patches. To provide a seamless environment, Wolverine supports 'hot-patching' of the generated repair patches, enabling the programmer to continue the debug session without requiring an abort-compile-debug cycle. We also propose new debug-repair possibilities, 'specification refinement' and 'specification slicing' made possible by Wolverine. We evaluate our framework on 1600 buggy programs (generated using fault injection) on a variety of data-structures like singly, doubly and circular linked-lists, Binary Search Trees, AVL trees, Red-Black trees and Splay trees; Wolverine could repair all the buggy instances within reasonable time (less than 5 sec in most cases). We also evaluate Wolverine on 247 (buggy) student submissions; Wolverine could repair more than 80% of programs where the student had made a reasonable attempt.","tags":["Other"],"title":"Synergistic debug-repair of heap manipulations","type":"publication"}]