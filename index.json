[{"authors":null,"categories":null,"content":"Hello! I am a PhD student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, Seattle. I am interested in exploring ways to make Machine Learning trustworthy. I have explored explainability and fairness aspects of Machine Learning to make it trustworthy. I hope to continue exploring this area in future.\nI majored in Electrical Engineering from IIT Kanpur and graduated with a minor in Computer Science in 2019. At IIT Kanpur, I worked on research projects in Fairness in Machine Learning and Program Analysis of Machine Learning and C code. In my free time, I enjoy reading blogs and books, connecting with friends, and exploring outdoors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://vsahil.github.io/author/sahil-verma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sahil-verma/","section":"authors","summary":"Hello! I am a PhD student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, Seattle. I am interested in exploring ways to make Machine Learning trustworthy.","tags":null,"title":"Sahil Verma","type":"authors"},{"authors":["Sahil Verma","Keegan Hines","John Dickerson"],"categories":["Explainability in ML"],"content":"","date":1623702333,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623702333,"objectID":"c977cc80ac7b1a899ad60ce48692ab5b","permalink":"https://vsahil.github.io/post/amortized-counterfactuals2021/","publishdate":"2021-06-15T01:55:33+05:30","relpermalink":"/post/amortized-counterfactuals2021/","section":"post","summary":" We present a novel technique to generate counterfactual explanations by formulating the problem into an MDP and then use planning or Reinforcement Learning approaches to learn a policy. The learned policy can then generate counterfactuals for all datapoints from the training distribution using only inference, and is thus amortized. This is the first approach that satisfies all desirable desiderata of counterfactual explanations. [Link to the post](https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations). ","tags":["Explainability in ML"],"title":"Amortized Counterfactual Explanations","type":"post"},{"authors":["Sahil Verma","John Dickerson","Keegan Hines"],"categories":["Explainability in ML"],"content":"","date":1623702126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623702126,"objectID":"6ea55723a38994236918ca98a0e86809","permalink":"https://vsahil.github.io/post/counterfactuals-survey/","publishdate":"2021-06-15T01:52:06+05:30","relpermalink":"/post/counterfactuals-survey/","section":"post","summary":" Within the field of explainable AI (XAI), the technique of counterfactual explainability has progressed rapidly, with many exciting developments just in the past couple years. To help crystallize and understand the major development areas, we presented a new paper at the NeurIPS Workshop on ML Retrospectives, Surveys, and Meta-Analyses. [Link to the post](https://www.arthur.ai/blog/counterfactual-explainability-neurips). ","tags":["Explainability in ML"],"title":"Reinforcement Learning for Counterfactual Explanations","type":"post"},{"authors":["Sahil Verma","John P. Dickerson","Keegan Hines"],"categories":["Explainability in ML"],"content":"","date":1623628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623628800,"objectID":"56448722e498707c37b9b2c2897bfcea","permalink":"https://vsahil.github.io/publication/hcxai-chi2021-cfe-challenges/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/hcxai-chi2021-cfe-challenges/","section":"publication","summary":"Counterfactual explanations (CFEs) are an emerging tech- nique under the umbrella of interpretability of machine learning (ML) models. They provide “what if” feedback of the form “if an input datapoint were x′ instead of x, then an ML model’s output would be y′ instead of y.” Counterfac- tual explainability for ML models has yet to see widespread adoption in industry. In this short paper, we posit reasons for this slow uptake. Leveraging recent work outlining desir- able properties of CFEs and our experience running the ML wing of a model monitoring startup, we identify outstanding obstacles hindering CFE deployment in industry.","tags":["Explainability in ML"],"title":"Counterfactual Explanations for Machine Learning: Challenges Revisited","type":"publication"},{"authors":["Sahil Verma","Aditya Lahiri","John P. Dickerson","Su-In Lee"],"categories":["Explainability in ML"],"content":"","date":1623628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623628800,"objectID":"e9cc84a50a9f588d616bba112132990b","permalink":"https://vsahil.github.io/publication/journe2021-pitfalls-xai/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/journe2021-pitfalls-xai/","section":"publication","summary":"As machine learning (ML) systems take a more prominent and central role in contributing to life-impacting decisions, ensuring their trustworthiness and accountability is of utmost importance. Explanations sit at the core of these desirable attributes of a ML system. The emerging field is frequently called ``Explainable AI (XAI)'' or ``Explainable ML.'' The goal of explainable ML is to intuitively explain the predictions of a ML system, while adhering to the needs to various stakeholders. Many explanation techniques were developed with contributions from both academia and industry. However, there are several existing challenges that have not garnered enough interest and serve as roadblocks to widespread adoption of explainable ML. In this short paper, we enumerate challenges in explainable ML from an industry perspective. We hope these challenges will serve as promising future research directions, and would contribute to democratizing explainable ML.","tags":["Explainability in ML"],"title":"Pitfalls of Explainable ML: An Industry Perspective","type":"publication"},{"authors":["Sahil Verma","Keegan Hines","John P. Dickerson"],"categories":["Explainability in ML"],"content":"","date":1623024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623024000,"objectID":"c0c0b84351b2f117f874604cdfd45e91","permalink":"https://vsahil.github.io/publication/amortized-counterfactuals2021/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/amortized-counterfactuals2021/","section":"publication","summary":"Explainable machine learning (ML) has gained traction in recent years due to the increasing adoption of ML-based systems in many sectors. Counterfactual explanations (CFEs) provide “what if” feedback of the form “if an input datapoint were x′ instead of x, then an ML-based system’s output would be y′ instead of y.” CFEs are attractive due to their actionable feedback, amenability to existing legal frameworks, and fidelity to the underlying ML model. Yet, current CFE approaches are single shot—that is, they assume x can change to x′ in a single time period. We propose a novel stochastic-control-based approach that generates sequential CFEs, that is, CFEs that allow x to move stochastically and sequentially across intermediate states to a final state x′. Our approach is model agnostic and black box. Furthermore, calculation of CFEs is amortized such that once trained, it applies to multiple datapoints without the need for re-optimization. In addition to these primary characteristics, our approach admits optional desiderata such as adherence to the data manifold, respect for causal relations, and sparsity—identified by past research as desirable properties of CFEs. We evaluate our approach using three real-world datasets and show successful generation of sequential CFEs that respect other counterfactual desiderata.","tags":["Explainability in ML"],"title":"Amortized Generation of Sequential Counterfactual Explanations for Black-box Models","type":"publication"},{"authors":["Sahil Verma","Michael Ernst","Rene Just"],"categories":["Fairness in ML"],"content":"","date":1612483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612483200,"objectID":"f5ead35a64004b1c1ef7b9aee40e7716","permalink":"https://vsahil.github.io/publication/removing-fairness-paper/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/removing-fairness-paper/","section":"publication","summary":"Machine learning systems are often trained using data collected from historical decisions. If past decisions were biased, then automated systems that learn from historical data will also be biased. We propose a black-box approach to identify and remove biased training data. Machine learning models trained on such debiased data (a subset of the original training data) have low individual discrimination, often 0%. These models also have greater accuracy and lower statistical disparity than models trained on the full historical data. We evaluated our methodology in experiments using 6 real-world datasets. Our approach outperformed seven previous approaches in terms of individual discrimination and accuracy.","tags":["Fairness in ML"],"title":"Removing biased data to improve fairness and accuracy","type":"publication"},{"authors":["Sahil Verma","Zhendong Su"],"categories":["Other"],"content":"","date":1606348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606348800,"objectID":"bea00ed1fd2d7d515f40d8627b5a5fd9","permalink":"https://vsahil.github.io/publication/shapeflow-eth-zurich/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/shapeflow-eth-zurich/","section":"publication","summary":"We present ShapeFlow, a dynamic abstract interpreter for TensorFlow which quickly catches tensor shape incompatibility errors, one of the most common bugs in deep learning code. ShapeFlow shares the same APIs as TensorFlow but only captures and emits tensor shapes, its abstract domain. ShapeFlow constructs a custom shape computational graph, similar to the computational graph used by TensorFlow. ShapeFlow requires no code annotation or code modification by the programmer, and therefore is convenient to use. We evaluate ShapeFlow on 52 programs collected by prior empirical studies to show how fast and accurately it can catch shape incompatibility errors compared to TensorFlow. We use two baselines: a worst-case training dataset size and a more realistic dataset size. ShapeFlow detects shape incompatibility errors highly accurately--with no false positives and a single false negative--and highly efficiently--with an average speed-up of 499X and 24X for the first and second baseline, respectively. We believe ShapeFlow is a practical tool that benefits machine learning developers. We will open-source ShapeFlow on GitHub to make it publicly available to both the developer and research communities.","tags":null,"title":"ShapeFlow: Dynamic Shape Interpreter for TensorFlow","type":"publication"},{"authors":["Sahil Verma","John Dickerson","Keegan Hines"],"categories":["Explainability in ML"],"content":"","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"f622e5b4e3adab600196c578a6db9b8d","permalink":"https://vsahil.github.io/publication/counterfactual-survey/","publishdate":"2020-11-27T13:37:35+05:30","relpermalink":"/publication/counterfactual-survey/","section":"publication","summary":"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.","tags":["Explainability in ML"],"title":"Counterfactual Explanations for Machine Learning: A Review","type":"publication"},{"authors":["Sahil Verma","Ruoyuan Gao","Chirag Shah"],"categories":["Fairness in ML"],"content":"","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586822400,"objectID":"afcad0cf815769b4c3ad562c3c04c0e7","permalink":"https://vsahil.github.io/publication/recommendation-fairness-definitions/","publishdate":"2020-11-27T13:37:22+05:30","relpermalink":"/publication/recommendation-fairness-definitions/","section":"publication","summary":"Several recent works have highlighted how search and recommender systems exhibit bias along different dimensions. Counteracting this bias and bringing a certain amount of fairness in search is crucial to not only creating a more balanced environment that considers relevance and diversity but also providing a more sustainable way forward for both content consumers and content producers. This short paper examines some of the recent works to define relevance, diversity, and related concepts. Then, it focuses on explaining the emerging concept of fairness in various recommendation settings. In doing so, this paper presents comparisons and highlights contracts among various measures, and gaps in our conceptual and evaluative frameworks.","tags":["Fairness in ML"],"title":"Recommendation Fairness Definitions","type":"publication"},{"authors":["Sahil Verma","Roland Yap"],"categories":["Other"],"content":"","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572825600,"objectID":"92a2e64ac77da85d1557c3c1b6920e16","permalink":"https://vsahil.github.io/publication/nus-paper/","publishdate":"2020-11-27T13:37:10+05:30","relpermalink":"/publication/nus-paper/","section":"publication","summary":"Symbolic execution is a powerful technique for bug finding and program testing. It is successful in finding bugs in real-world code. The core reasoning techniques use constraint solving, path exploration, and search, which are also the same techniques used in solving combinatorial problems, e.g., finite-domain constraint satisfaction problems (CSPs). We propose CSP instances as more challenging benchmarks to evaluate the effectiveness of the core techniques in symbolic execution. We transform CSP benchmarks into C programs suitable for testing the reasoning capabilities of symbolic execution tools. From a single CSP P, we transform P depending on transformation choice into different C programs. Preliminary testing with the KLEE, Tracer-X, and LLBMC tools show substantial runtime differences from transformation and solver choice. Our C benchmarks are effective in showing the limitations of existing symbolic execution tools. The motivation for this work is we believe that benchmarks of this form can spur the development and engineering of improved core reasoning in symbolic execution engines.","tags":["Other"],"title":"Benchmarking Symbolic Execution Using Constraint Problems - Initial Results","type":"publication"},{"authors":["Jesse Michel","Sahil Verma","Benjamin Sherman","Michael Carbin"],"categories":["Other"],"content":"","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561161600,"objectID":"797342b20fc3fd9c20ad28029a56a81f","permalink":"https://vsahil.github.io/publication/mit-nap-workshop/","publishdate":"2020-11-27T13:37:42+05:30","relpermalink":"/publication/mit-nap-workshop/","section":"publication","summary":"Low-precision approximation of programs enables faster computation in fields such as machine learning, data analytics, and vision. Such approximations automatically transform a program into one that approximates the original output but executes much faster. At the heart of this approximation is sensitivity analysis - understanding the program's robustness to various perturbations. Sensitivity analysis provides a metric to measure how much each value in the program to produce a faster, yet accurate, approximate program. We propose NAP (Noise-based Analyzer of Programs) which provides a novel sensitivity analysis of each operator and variable in a program. NAP performs sensitivity analysis by introducing independent Gaussian noise to each value in a program (e.g., arithmetic operator and variable reference), producing a stochastic semantics of the program.","tags":["Other"],"title":"NAP: Noise-Based Sensitivity Analysis for Programs","type":"publication"},{"authors":["Sahil Verma","Julia Rubin"],"categories":["Fairness in ML"],"content":"","date":1527552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527552000,"objectID":"39b073e1a28aeb4d79504e32dbb98d29","permalink":"https://vsahil.github.io/publication/fairness-definitions/","publishdate":"2020-11-27T13:36:15+05:30","relpermalink":"/publication/fairness-definitions/","section":"publication","summary":"Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.","tags":["Fairness in ML"],"title":"Fairness Definitions Explained","type":"publication"},{"authors":["Sahil Verma","Subhajit Roy"],"categories":["Other"],"content":"","date":1503273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503273600,"objectID":"5eacd4c918a096ca9eaaa6c0254239ee","permalink":"https://vsahil.github.io/publication/sdb-wolverine1/","publishdate":"2020-11-27T13:37:02+05:30","relpermalink":"/publication/sdb-wolverine1/","section":"publication","summary":"We present Wolverine, an integrated Debug-Repair environment for heap manipulating programs. Wolverine facilitates stepping through a concrete program execution, provides visualizations of the abstract program states (as box-and-arrow diagrams) and integrates a novel, proof-directed repair algorithm to synthesize repair patches. To provide a seamless environment, Wolverine supports 'hot-patching' of the generated repair patches, enabling the programmer to continue the debug session without requiring an abort-compile-debug cycle. We also propose new debug-repair possibilities, 'specification refinement' and 'specification slicing' made possible by Wolverine. We evaluate our framework on 1600 buggy programs (generated using fault injection) on a variety of data-structures like singly, doubly and circular linked-lists, Binary Search Trees, AVL trees, Red-Black trees and Splay trees; Wolverine could repair all the buggy instances within reasonable time (less than 5 sec in most cases). We also evaluate Wolverine on 247 (buggy) student submissions; Wolverine could repair more than 80% of programs where the student had made a reasonable attempt.","tags":["Other"],"title":"Synergistic debug-repair of heap manipulations","type":"publication"}]